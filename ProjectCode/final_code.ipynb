{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3765730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#openCV\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis,skew\n",
    "#scipy is the statistical elder brother of numpy\n",
    "import os\n",
    "#os helps in fetching files from different directories i.e. communication with the operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d0f80046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Feature_vector:\n",
    "#     #As given in  https://arxiv.org/abs/2004.07941, create the motion, location and appearance metric for the objects\n",
    "#     motion=[]\n",
    "#     location=[]\n",
    "#     appearance=[]\n",
    "    \n",
    "#     #weight corresponding to the motion metric\n",
    "#     w1=1\n",
    "    \n",
    "#     #weight corresponding to the location metric\n",
    "#     w2=0.4\n",
    "    \n",
    "#     #weight corresponding to the appearance metric\n",
    "#     w3=0.9\n",
    "    \n",
    "#     #constructor in python\n",
    "#     def __init__(self,motion,location,appearance):\n",
    "#         self.motion=motion\n",
    "#         self.location=location\n",
    "#         self.appearance=appearance\n",
    "    \n",
    "#     #overloading multiplication operator\n",
    "#     def __sub__(self, other):\n",
    "#         return np.array(self.w1*(np.array(self.motion)-np.array(other.motion)),self.w2*(np.array(self.location)-np.array(other.location)),self.w3*(np.array(self.appearance)-np.array(other.appearance)))\n",
    "        \n",
    "#     #change weighting  \n",
    "#     def set_weights(self,w11,w22,w33):\n",
    "#         self.w1=w11\n",
    "#         self.w2=w22\n",
    "#         self.w3=w33\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bbe97ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(img,boxes,idx):\n",
    "    #Extract a patch in the image corresponding to the object's bounding box in the frame.\n",
    "    imgs=[]\n",
    "    if len(idx)>0:\n",
    "        for i in idx.flatten():\n",
    "            center_x,center_y,w,h=boxes[i]\n",
    "            # Using the center x, y coordinates to derive the top\n",
    "            # and the left corner of the bounding box\n",
    "            x=int(center_x-(w/2))\n",
    "            y=int(center_y-(h/2))\n",
    "            ##\n",
    "            #The following is done to deal with typical situation when x or y are going out of frame\n",
    "            #i.e not whole bounding box in the frame\n",
    "            \n",
    "            #stores part of the width to be cropped which is out of frame\n",
    "            subx=0\n",
    "            #stores part of the height to be cropped which is out of frame\n",
    "            suby=0\n",
    "            \n",
    "            if x<0 or x>=img.shape[1]:\n",
    "                subx=x if x<0 else -1*x\n",
    "                x=0 if x<0 else img.shape[1]-1\n",
    "            if y<0 or y>=img.shape[0]:\n",
    "                suby=y if y<0 else -1*y\n",
    "                y=0 if y<0 else img.shape[0]-1\n",
    "            ##\n",
    "            imgs.append(img[y:y+int(h)+suby,x:x+int(w)+subx,:])\n",
    "    return imgs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c9c21343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_process(frames_path, feature_matrix):\n",
    "    frame_prev=None\n",
    "    #print(\"hello!\")\n",
    "    for img in os.listdir(frames_path):\n",
    "        \n",
    "        if img.endswith(\".jpg\"):\n",
    "            image_path=os.path.join(frames_path,img)\n",
    "            #print(frame_prev)\n",
    "            # RUNNING YOLOv4 OBJECT DETECTION FIRST\n",
    "            # returns a deep learning network using the yolov4 format\n",
    "            \n",
    "            net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
    "            # cv2.dnn.readNet=>https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422\n",
    "            # type net=Net object=>https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html\n",
    "            \n",
    "            # for running optical flow algorithm we need previous frame as well\n",
    "            if frame_prev is None:\n",
    "                frame_prev=cv2.imread(image_path)\n",
    "                #imread->https://www.geeksforgeeks.org/python-opencv-cv2-imread-method/\n",
    "                continue\n",
    "            \n",
    "            frame=cv2.imread(image_path)\n",
    "            \n",
    "            (height, width, _) = frame.shape\n",
    "            \n",
    "            # preprocessing the frame before feeding it to the neural net.\n",
    "            # scale the pixel values to 1/255=>1/255\n",
    "            # Resizing frame to (416,416) pixels as yolov4 architecture works on frame of that size.\n",
    "            # No mean supplied to the three R,G,B channels=>(0,0,0)\n",
    "            # OpenCV assumes images are in BGR channel order, thus we must swap the R and B channels of the original RGB frame=> swapRB=true\n",
    "            # No cropping of the frame=>crop=False\n",
    "            blob = cv2.dnn.blobFromImage(frame,1 / 255,(416, 416),(0, 0, 0),swapRB=True,crop=False)\n",
    "            # above parameters are needed for yolov4 detection!\n",
    "            # cv2.dnn.blobFromImage=> creates processed 4-dimensional blob for use in our neural net.Further info=> https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "\n",
    "            # setting input to neural net\n",
    "            net.setInput(blob)\n",
    "\n",
    "            # net.getUnconnectedOutLayers(): It gives you the final layers number in the list from net.getLayerNames().\n",
    "            output_layer_names = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "            # Runs forward pass to compute output of layer with name outputName\n",
    "            layerOutputs = net.forward(output_layer_names)\n",
    "\n",
    "            # model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\n",
    "            # There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\n",
    "\n",
    "            # boxes stores the location properties of detected objects\n",
    "            # confidences stores the confidence score of detecting that object\n",
    "\n",
    "            boxes = []\n",
    "            confidences = []\n",
    "            appearance = []\n",
    "\n",
    "            # traversing through the 3 outputs at varying resolution\n",
    "\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                    # detection holds the location(first 5 elements) and class probabilities(rest 80 elements)\n",
    "                    scores = detection[5:]\n",
    "\n",
    "                    # selecting the index of maximum class probabilty of total 80 classes and its probability value as well\n",
    "\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    # check to filter objects we are entirely sure belong to some class\n",
    "\n",
    "                    if confidence > 0.6:\n",
    "\n",
    "                        # Extracting the center coordinates of the bounding box\n",
    "                        # we need to convert back to original dimensions thus multiply by width,height is important\n",
    "                        # print(detection[0])\n",
    "\n",
    "                        center_x = detection[0] * width\n",
    "                        center_y = detection[1] * height\n",
    "\n",
    "                        # extracting the width and height of bounding box\n",
    "\n",
    "                        w = detection[2] * width\n",
    "                        h = detection[3] * height\n",
    "                        boxes.append([center_x, center_y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        appearance.append(scores)\n",
    "\n",
    "            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.6, 0.4)\n",
    "\n",
    "            # NMSBoxes=>https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
    "\n",
    "            i = 0\n",
    "            if boxes != []:\n",
    "\n",
    "                # extract image corresponsing to bounding box coordinates for previous frame\n",
    "\n",
    "                patch_prev = get_patch(frame_prev, boxes, indexes)\n",
    "\n",
    "                # extract image corresponding to bounding box coordinates for current frame\n",
    "\n",
    "                patch = get_patch(frame, boxes, indexes)\n",
    "\n",
    "                if patch_prev != [] and patch != []:\n",
    "\n",
    "                    # looping through objects detected to find their OPTICAL FLOW\n",
    "\n",
    "                    for (prev, actual) in zip(patch_prev, patch):\n",
    "\n",
    "                        # Preprocessing to gray scale\n",
    "\n",
    "                        prvs = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "                        next_ = cv2.cvtColor(actual, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                        # ##TRIAL--flow = np.zeros([prev.shape[0],prev.shape[1],2])\n",
    "                        # ##TRIAL--cv2.optflow.calcOpticalFlowDenseRLOF(prev, actual,flow,None)\n",
    "\n",
    "                        # Dense optical flow algorithm\n",
    "\n",
    "                        flow = cv2.calcOpticalFlowFarneback(prvs,next_,None,0.5,3,15,3,5,1.2,0,)\n",
    "\n",
    "                        # getting magnitude of velocity vectors of optical flow\n",
    "\n",
    "                        (mag, _) = cv2.cartToPolar(flow[..., 0],flow[..., 1])\n",
    "                        \n",
    "                        # now creating feature vector for the object\n",
    "                        # computing mean, variance, kurtosis and skew of the magnitude of velocity vectors\n",
    "                        # motion metric\n",
    "\n",
    "                        mean = np.mean(mag)\n",
    "                        variance = np.var(mag)\n",
    "                        kurtosis_ = kurtosis(mag, None)\n",
    "                        skew_ = skew(mag, None)\n",
    "\n",
    "                        #location metric\n",
    "                        (cx, cy, wi, hi) = boxes[indexes.flatten()[i]]\n",
    "\n",
    "                        \n",
    "                        # appearance metric->appearance[indexes.flatten()[i]]\n",
    "\n",
    "                        # Appending feature vector of given object to feature matrix along with weight\n",
    "                        #1-> weight fot motion metric 0.4-> weight for location metric o.9-> weight for appearance metric\n",
    "                        feature_matrix.append([mean,variance,kurtosis_,skew_]+[0.4*cx,0.4*cy,0.4*wi*hi]+list(0.9*appearance[indexes.flatten()[i]]))\n",
    "                        \n",
    "                        i = i + 1\n",
    "\n",
    "            # updating previous frame\n",
    "\n",
    "            frame_prev = frame\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "db4358bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid_process(video_path,feature_matrix):\n",
    "    #get video capture object for the camera for the specified video file \n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    \n",
    "    #RUNNING YOLOv4 OBJECT DETECTION FIRST\n",
    "    \n",
    "    #returns a deep learning network using the yolov4 format \n",
    "    net=cv2.dnn.readNet('yolov4.weights','yolov4.cfg')\n",
    "    #cv2.dnn.readNet=>https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422\n",
    "    #type net=Net object=>https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html\n",
    "    \n",
    "    #Sometimes, cap may not have initialized the capture. \n",
    "    #You can check whether it is initialized or not by the method cap.isOpened().\n",
    "    if(cap.isOpened()==False): \n",
    "        print(\"Error connecting to camera\")\n",
    "        return -1\n",
    "    \n",
    "    #for running optical flow algorithm we need previous frame as well\n",
    "    ret,frame_prev=cap.read()\n",
    "    #cap.read() returns a bool (True/False) which is stored in \"ret\" here. \n",
    "    #If the frame is read correctly, it will be True    \n",
    "    \n",
    "    if ret==False:\n",
    "        print(\"Error loading frame\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        ret,frame=cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            #storing original frame dimensions\n",
    "            height,width,_=frame.shape\n",
    "            \n",
    "            #preprocessing the frame before feeding it to the neural net. \n",
    "            #scale the pixel values to 1/255=>1/255\n",
    "            #Resizing frame to (416,416) pixels as yolov4 architecture works on frame of that size.\n",
    "            #No mean supplied to the three R,G,B channels=>(0,0,0)\n",
    "            #OpenCV assumes images are in BGR channel order, thus we must swap the R and B channels of the original RGB frame=> swapRB=true\n",
    "            #No cropping of the frame=>crop=False\n",
    "            blob=cv2.dnn.blobFromImage(frame,1/255,(416,416),(0,0,0),swapRB=True,crop=False)\n",
    "            #above parameters are needed for yolov4 detection!\n",
    "            #cv2.dnn.blobFromImage=> creates processed 4-dimensional blob for use in our neural net.Further info=> https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "            \n",
    "            #setting input to neural net\n",
    "            net.setInput(blob)\n",
    "            \n",
    "            #net.getUnconnectedOutLayers(): It gives you the final layers number in the list from net.getLayerNames().\n",
    "            output_layer_names=net.getUnconnectedOutLayersNames()\n",
    "            \n",
    "            #Runs forward pass to compute output of layer with name outputName\n",
    "            layerOutputs=net.forward(output_layer_names)\n",
    "            #model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\n",
    "            #There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\n",
    "            \n",
    "            #boxes stores the location properties of detected objects\n",
    "            #confidences stores the confidence score of detecting that object\n",
    "            boxes=[]\n",
    "            confidences=[]\n",
    "            appearance=[]\n",
    "            \n",
    "            #traversing through the 3 outputs at varying resolution\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                    #detection holds the location(first 5 elements) and class probabilities(rest 80 elements)\n",
    "                    scores=detection[5:]\n",
    "                    #print(scores)\n",
    "                    #selecting the index of maximum class probabilty of total 80 classes and its probability value as well\n",
    "                    class_id=np.argmax(scores)\n",
    "                    confidence=scores[class_id]\n",
    "                    #check to filter objects we are entirely sure belong to some class\n",
    "                    if(confidence>0.6):\n",
    "                        #Extracting the center coordinates of the bounding box\n",
    "                        #we need to convert back to original dimensions thus multiply by width,height is important\n",
    "                        #print(detection[0])\n",
    "                        center_x=detection[0]*width\n",
    "                        center_y=detection[1]*height\n",
    "                        #extracting the width and height of bounding box\n",
    "                        w=detection[2]*width\n",
    "                        h=detection[3]*height\n",
    "                        boxes.append([center_x,center_y,w,h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        appearance.append(scores)\n",
    "            \n",
    "            indexes=cv2.dnn.NMSBoxes(boxes,confidences,0.6,0.4)\n",
    "            #NMSBoxes=>https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
    "            i=0\n",
    "            if boxes!=[]:\n",
    "                #extract image corresponsing to bounding box coordinates for previous frame\n",
    "                patch_prev=get_patch(frame_prev,boxes,indexes)\n",
    "                #extract image corresponding to bounding box coordinates for current frame\n",
    "                patch=get_patch(frame,boxes,indexes)\n",
    "                \n",
    "                if patch_prev!=[] and patch!=[]:\n",
    "                    #looping through objects detected to find their OPTICAL FLOW\n",
    "                    for prev,actual in zip(patch_prev,patch):\n",
    "                        #Preprocessing to gray scale\n",
    "                        prvs  = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "                        next_ = cv2.cvtColor(actual, cv2.COLOR_BGR2GRAY)\n",
    "                        \n",
    "                        ###TRIAL--flow = np.zeros([prev.shape[0],prev.shape[1],2])\n",
    "                        ###TRIAL--cv2.optflow.calcOpticalFlowDenseRLOF(prev, actual,flow,None)\n",
    "                        \n",
    "                        #Dense optical flow algorithm\n",
    "                        flow=cv2.calcOpticalFlowFarneback(prvs, next_, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                        #getting magnitude of velocity vectors of optical flow \n",
    "                        mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                        #computing mean, variance, kurtosis and skew of the magnitude of velocity vectors\n",
    "                        #motion metric\n",
    "                        \n",
    "                        mean=np.mean(mag)\n",
    "                        variance=np.var(mag)\n",
    "                        kurtosis_=kurtosis(mag,None)\n",
    "                        skew_=skew(mag,None)\n",
    "                        \n",
    "                        #location metric\n",
    "                        cx,cy,wi,hi=boxes[indexes.flatten()[i]]\n",
    "                        \n",
    "                        #appearance[indexes.flatten()[i]]-> appearance metric\n",
    "                        \n",
    "                        #now creating feature vector for the object and appending feature vector of given object to feature matrix along with their weight\n",
    "                        feature_matrix.append([mean,variance,kurtosis_,skew_]+[0.4*cx,0.4*cy,0.4*wi*hi]+list(0.9*appearance[indexes.flatten()[i]]))\n",
    "                        \n",
    "                        i=i+1\n",
    "                        \n",
    "                        \n",
    "            #updating previous frame            \n",
    "            frame_prev=frame\n",
    "            key=cv2.waitKey(1)\n",
    "            \n",
    "            #cv2.waitKey([delay])=>The function waitKey waits for a key event infinitely and the delay is in milliseconds. waitKey(0) means forever.\n",
    "            #For more details=>https://stackoverflow.com/questions/57690899/how-cv2-waitkey1-0xff-ordq-works\n",
    "            \n",
    "            #if pressed key has ASCII value 27 i.e q\n",
    "            if(key==27):\n",
    "                break\n",
    "            \n",
    "            #if len(indexes)>0:\n",
    "            #for i in indexes.flatten():                        \n",
    "            \n",
    "        else:\n",
    "            print(\"Error loading frame or End of frames\")\n",
    "            return\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fc3ab3ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-6aff47a96224>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#os.listdir->https://www.geeksforgeeks.org/python-os-listdir-method/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".avi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mvid_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Using the SHANGAI TECH DATASET training videos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-204-7538c41eb155>\u001b[0m in \u001b[0;36mvid_process\u001b[1;34m(video_path, feature_matrix)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m#Runs forward pass to compute output of layer with name outputName\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mlayerOutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;31m#model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING THE DATASET\n",
    "train_vector=[]\n",
    "#Getting the parent directory\n",
    "parent_directory=os.path.dirname(os.getcwd())\n",
    "#os.path.dirname() method in Python is used to get directory name from the specified path \n",
    "#i.e the directory that hold the current file\n",
    "#Python method os.getcwd() returns current working directory of a process.\n",
    "#For more info->https://www.geeksforgeeks.org/python-os-path-dirname-method/\n",
    "\n",
    "#accesing the DATASET directory\n",
    "direc=os.path.join(parent_directory,'Dataset')\n",
    "#os.path.join-> https://www.geeksforgeeks.org/python-os-path-join-method/\n",
    "\n",
    "#first using the AVENUE DATASET training videos for training\n",
    "curr_direc=os.path.join(direc,'Avenue Dataset','training_videos')\n",
    "for filename in os.listdir(curr_direc):\n",
    "    #os.listdir->https://www.geeksforgeeks.org/python-os-listdir-method/\n",
    "    if filename.endswith(\".avi\"):\n",
    "        vid_process(filename,train_vector)\n",
    "\n",
    "#Using the SHANGAI TECH DATASET training videos\n",
    "curr_direc=os.path.join(direc,'ShangaiTech_training','videos')\n",
    "for filename in os.listdir(curr_direc):\n",
    "    if filename.endswith(\".avi\"):\n",
    "        vid_process(filename,train_vector)\n",
    "\n",
    "#using the PEDESTRIAN training dataset\n",
    "curr_direc=os.path.join(direc,'ped2','training','frames')\n",
    "for subdirec in os.listdir(curr_direc):\n",
    "    video_frames=os.path.join(curr_direc,subdirec)\n",
    "    frames_process(video_frames,train_vector)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5d6cad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.matlib\n",
    "def knndis(t,M2):\n",
    "    #A vectorised implementation of finding KNN distance between a single point and the M2 matrix\n",
    "    Mg=M2.shape[0]\n",
    "    #this will replicate the feature vector of 't' Mg times columnwise. \n",
    "    #To subtract, transpose is taken to make it compatible\n",
    "    #Sum is taken of each row\n",
    "    \n",
    "    dist=np.sqrt(np.sum((np.matlib.repmat(t,Mg,1) - M2)**2,1))\n",
    "    #np.matlib.repmat->Repeat given matrix columnwise(mg) and rowise(1)-> https://numpy.org/doc/stable/reference/generated/numpy.matlib.repmat.html\n",
    "    \n",
    "    #sorting the distance according to ascending order\n",
    "    np.sort(dist)\n",
    "    \n",
    "    #return the sum of first 10 neighbours i.e k=10\n",
    "    return sum(dist[0:10])/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0d97a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 87)\n"
     ]
    }
   ],
   "source": [
    "#randomly shuffling the feature matrix\n",
    "#print(train_vector)\n",
    "np.random.shuffle(train_vector)\n",
    "#random.shuffle-> shuffle contents of array inplace along the first axis of multi-array. \n",
    "#for more details->https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html\n",
    "\n",
    "#converting to np array for later modifications\n",
    "train_feature_matrix=np.array(train_vector)\n",
    "print(train_feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ff319346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[883.7077792962497, 396.3700198480914, 442.40959393533757, 670.6719989592364, 865.687404399473, 441.8226413471666, 447.5816666494525, 317.9718373536208, 947.8824303364445, 867.1001928386437, 430.7952978723947, 408.06973200066153, 478.87632602601326, 323.57545279562567, 499.0507611746365, 410.706906308484, 445.2896375700378, 729.6477740388287, 741.5830718975882, 321.0911510585207, 574.3725034822593, 318.41728657403837, 428.90380924121354, 477.274023267278, 318.4270384303595, 749.8200466952475, 433.8716857352285, 389.95606809854127, 421.9616718223666, 370.41530574148805, 347.965687357525, 415.8275133817474, 327.60505837615347, 429.02080797413737, 738.4367964014381, 964.2275917462114, 366.75746294772694, 325.53717129320523, 1086.531373027556, 434.6268374600854, 325.6347910519639, 424.02739859735794, 362.0397059965531, 327.5908335170965, 865.0592799988204, 326.88727998133635, 442.16517857231236, 402.30021152229347, 605.4003855613915, 915.4502272763787, 866.4448517724229, 715.2528864342041, 848.3557586255504, 321.6982886525183, 446.07520609769864, 323.740438994134, 423.35523746618884, 824.9439050569538, 318.46110318482374, 326.49088110869366, 561.8694597077621, 728.294191242247, 918.6855736434243, 873.8231208531454, 426.19344186199794, 333.3220974623717, 335.0934484948601, 341.2139459593216, 831.7045578236872, 326.2591810333238, 424.418906539264, 321.4231250806328, 424.1950818855331, 567.3902142636742, 850.3071694684455, 424.9645498866357, 621.5785731307896, 324.4496072457482, 318.4513103864524, 432.33367540300986, 317.6417336287724, 401.676861313918, 345.80495453476664, 321.35034548339956, 386.6081009257823, 388.44899846220653, 431.1115587810871, 434.15193892096187, 871.3935310088176, 786.4023449098632, 762.902364265303, 326.87594364245695, 326.2749645019827, 474.1774510524315, 320.18787087392116, 325.4399671176205, 416.08413012189715, 390.52134107047175, 413.1214779771186, 340.07315777266996, 912.8474650962766, 324.28995988577856, 435.21909311011234, 395.8865906386667, 537.4153973062362, 318.5341510975578, 508.29011163683964, 317.91308447492185, 345.51341331190804, 389.55568120364853, 884.2104261448698, 326.1548466390447, 872.6552458097358, 318.36264791693696, 983.5167009537208, 333.10829483025407, 426.0588032464555, 406.04777181401994, 891.933713475323, 444.2747266391187, 877.2791289774725, 796.9522523396583]\n"
     ]
    }
   ],
   "source": [
    "#Keeps track of di for all object i. \n",
    "errors=list()\n",
    "\n",
    "#as given in the paper, split the training dataset into M1 and M2. Here 20% of training dataset goes to M1. \n",
    "p1=int(train_feature_matrix.shape[0]*0.2)\n",
    "p2=train_feature_matrix.shape[0]-p1\n",
    "\n",
    "M1=train_feature_matrix[0:p1]\n",
    "M2=train_feature_matrix[p1:None]\n",
    "\n",
    "#Finding the KNN distance for all the points in M1\n",
    "for i in range(p1):\n",
    "    #print(M1[i].shape)\n",
    "    errors.append(knndis(M1[i],M2)) \n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e90c6207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872.6552458097358\n"
     ]
    }
   ],
   "source": [
    "#Using base thresold as one where 90% of KNN distance is lesser\n",
    "Base_lm = np.sort(errors)[int(len(errors)*0.9)]\n",
    "print(Base_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a6749b7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-819630bf202d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1800\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.sort(errors)[1:1800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefac4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
