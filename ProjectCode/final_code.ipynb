{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3765730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#openCV\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis,skew\n",
    "#scipy is the statistical elder brother of numpy\n",
    "import os\n",
    "#os helps in fetching files from different directories i.e. communication with the operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbe97ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_patches(flow,boxes,idx):\n",
    "    #Extract a patch in the image corresponding to the object's bounding box in the frame.\n",
    "    flows=[]\n",
    "    if len(idx)>0:\n",
    "        for i in idx.flatten():\n",
    "            center_x,center_y,w,h=boxes[i]\n",
    "            # Using the center x, y coordinates to derive the top\n",
    "            # and the left corner of the bounding box\n",
    "            x=int(center_x-(w/2))\n",
    "            y=int(center_y-(h/2))\n",
    "            ##\n",
    "            #The following is done to deal with typical situation when x or y are going out of frame\n",
    "            #i.e not whole bounding box in the frame\n",
    "            \n",
    "            #stores part of the width to be cropped which is out of frame\n",
    "            subx=0\n",
    "            #stores part of the height to be cropped which is out of frame\n",
    "            suby=0\n",
    "            \n",
    "            if x<0 or x>=flow.shape[1]:\n",
    "                subx=x if x<0 else -1*x\n",
    "                x=0 if x<0 else flow.shape[1]-1\n",
    "            if y<0 or y>=flow.shape[0]:\n",
    "                suby=y if y<0 else -1*y\n",
    "                y=0 if y<0 else flow.shape[0]-1\n",
    "            ##\n",
    "            flows.append(flow[y:y+int(h)+suby,x:x+int(w)+subx,:])\n",
    "    return flows       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9c21343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_process(frames_path, feature_matrix):\n",
    "    frame_prev=None\n",
    "    #print(\"hello!\")\n",
    "    for img in os.listdir(frames_path):\n",
    "        \n",
    "        if img.endswith(\".jpg\"):\n",
    "            image_path=os.path.join(frames_path,img)\n",
    "            #print(frame_prev)\n",
    "            # RUNNING YOLOv4 OBJECT DETECTION FIRST\n",
    "            # returns a deep learning network using the yolov4 format\n",
    "            \n",
    "            net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
    "            # cv2.dnn.readNet=>https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422\n",
    "            # type net=Net object=>https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html\n",
    "            \n",
    "            # for running optical flow algorithm we need previous frame as well\n",
    "            if frame_prev is None:\n",
    "                frame_prev=cv2.imread(image_path)\n",
    "                #imread->https://www.geeksforgeeks.org/python-opencv-cv2-imread-method/\n",
    "                continue\n",
    "            \n",
    "            frame=cv2.imread(image_path)\n",
    "            \n",
    "            (height, width, _) = frame.shape\n",
    "            \n",
    "            #APPLYING OPTICAL FLOW ON THE FRAMES\n",
    "            #Preprocessing to gray scale\n",
    "            prvs=cv2.cvtColor(frame_prev,cv2.COLOR_BGR2GRAY)\n",
    "            curr=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            #Dense optical flow algorithm\n",
    "            flow=cv2.calcOpticalFlowFarneback(prvs, curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            \n",
    "            # preprocessing the frame before feeding it to the neural net.\n",
    "            # scale the pixel values to 1/255=>1/255\n",
    "            # Resizing frame to (416,416) pixels as yolov4 architecture works on frame of that size.\n",
    "            # No mean supplied to the three R,G,B channels=>(0,0,0)\n",
    "            # OpenCV assumes images are in BGR channel order, thus we must swap the R and B channels of the original RGB frame=> swapRB=true\n",
    "            # No cropping of the frame=>crop=False\n",
    "            blob = cv2.dnn.blobFromImage(frame,1 / 255,(416, 416),(0, 0, 0),swapRB=True,crop=False)\n",
    "            # above parameters are needed for yolov4 detection!\n",
    "            # cv2.dnn.blobFromImage=> creates processed 4-dimensional blob for use in our neural net.Further info=> https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "\n",
    "            # setting input to neural net\n",
    "            net.setInput(blob)\n",
    "\n",
    "            # net.getUnconnectedOutLayers(): It gives you the final layers number in the list from net.getLayerNames().\n",
    "            output_layer_names = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "            # Runs forward pass to compute output of layer with name outputName\n",
    "            layerOutputs = net.forward(output_layer_names)\n",
    "\n",
    "            # model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\n",
    "            # There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\n",
    "\n",
    "            # boxes stores the location properties of detected objects\n",
    "            # confidences stores the confidence score of detecting that object\n",
    "\n",
    "            boxes = []\n",
    "            confidences = []\n",
    "            appearance = []\n",
    "\n",
    "            # traversing through the 3 outputs at varying resolution\n",
    "\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                    # detection holds the location(first 5 elements) and class probabilities(rest 80 elements)\n",
    "                    scores = detection[5:]\n",
    "\n",
    "                    # selecting the index of maximum class probabilty of total 80 classes and its probability value as well\n",
    "\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    # check to filter objects we are entirely sure belong to some class\n",
    "\n",
    "                    if confidence > 0.6:\n",
    "\n",
    "                        # Extracting the center coordinates of the bounding box\n",
    "                        # we need to convert back to original dimensions thus multiply by width,height is important\n",
    "                        # print(detection[0])\n",
    "\n",
    "                        center_x = detection[0] * width\n",
    "                        center_y = detection[1] * height\n",
    "\n",
    "                        # extracting the width and height of bounding box\n",
    "\n",
    "                        w = detection[2] * width\n",
    "                        h = detection[3] * height\n",
    "                        boxes.append([center_x, center_y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        appearance.append(scores)\n",
    "\n",
    "            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.6, 0.4)\n",
    "\n",
    "            # NMSBoxes=>https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
    "\n",
    "            i = 0\n",
    "            if boxes != []:\n",
    "                # extract flow ptached corresponsing to bounding box coordinates for detected objects\n",
    "                flows=get_flow_patches(flow,boxes,indexes)\n",
    "                \n",
    "                if flows!=[]:\n",
    "                    for flow_patch in flows:\n",
    "                        mag, _ = cv2.cartToPolar(flow_patch[..., 0], flow_patch[..., 1])\n",
    "                        \n",
    "                        #computing mean, variance, kurtosis and skew of the magnitude of velocity vectors\n",
    "                        #motion metric\n",
    "                        mean=np.mean(mag)\n",
    "                        variance=np.var(mag)\n",
    "                        kurtosis_=kurtosis(mag,None)\n",
    "                        skew_=skew(mag,None)\n",
    "                        \n",
    "                        #location metric\n",
    "                        cx,cy,wi,hi=boxes[indexes.flatten()[i]]\n",
    "                        \n",
    "                        #appearance[indexes.flatten()[i]]-> appearance metric\n",
    "                        \n",
    "                        #now creating feature vector for the object and appending feature vector of given object to feature matrix along with their weight\n",
    "                        feature_matrix.append([mean,variance,kurtosis_,skew_]+[0.4*cx,0.4*cy,0.4*wi*hi]+list(0.9*appearance[indexes.flatten()[i]]))\n",
    "                        \n",
    "                        i=i+1\n",
    "               \n",
    "\n",
    "            # updating previous frame\n",
    "\n",
    "            frame_prev = frame\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db4358bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid_process(video_path,feature_matrix):\n",
    "    #get video capture object for the camera for the specified video file \n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    \n",
    "    #returns a deep learning network using the yolov4 format \n",
    "    net=cv2.dnn.readNet('yolov4.weights','yolov4.cfg')\n",
    "    #cv2.dnn.readNet=>https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422\n",
    "    #type net=Net object=>https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html\n",
    "    \n",
    "    #Sometimes, cap may not have initialized the capture. \n",
    "    #You can check whether it is initialized or not by the method cap.isOpened().\n",
    "    if(cap.isOpened()==False): \n",
    "        print(\"Error connecting to camera\")\n",
    "        return -1\n",
    "    \n",
    "    #for running optical flow algorithm we need previous frame as well\n",
    "    ret,frame_prev=cap.read()\n",
    "    #cap.read() returns a bool (True/False) which is stored in \"ret\" here. \n",
    "    #If the frame is read correctly, it will be True    \n",
    "    \n",
    "    if ret==False:\n",
    "        print(\"Error loading frame\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        ret,frame=cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            #storing original frame dimensions\n",
    "            height,width,_=frame.shape\n",
    "            \n",
    "            #APPLYING OPTICAL FLOW ON THE FRAMES\n",
    "            #Preprocessing to gray scale\n",
    "            prvs=cv2.cvtColor(frame_prev,cv2.COLOR_BGR2GRAY)\n",
    "            curr=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            #Dense optical flow algorithm\n",
    "            flow=cv2.calcOpticalFlowFarneback(prvs, curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            \n",
    "            #RUNNING YOLOv4 OBJECT DETECTION now\n",
    "            #preprocessing the frame before feeding it to the neural net. \n",
    "            #scale the pixel values to 1/255=>1/255\n",
    "            #Resizing frame to (416,416) pixels as yolov4 architecture works on frame of that size.\n",
    "            #No mean supplied to the three R,G,B channels=>(0,0,0)\n",
    "            #OpenCV assumes images are in BGR channel order, thus we must swap the R and B channels of the original RGB frame=> swapRB=true\n",
    "            #No cropping of the frame=>crop=False\n",
    "            blob=cv2.dnn.blobFromImage(frame,1/255,(416,416),(0,0,0),swapRB=True,crop=False)\n",
    "            #above parameters are needed for yolov4 detection!\n",
    "            #cv2.dnn.blobFromImage=> creates processed 4-dimensional blob for use in our neural net.Further info=> https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "            \n",
    "            #setting input to neural net\n",
    "            net.setInput(blob)\n",
    "            \n",
    "            #net.getUnconnectedOutLayers(): It gives you the final layers number in the list from net.getLayerNames().\n",
    "            output_layer_names=net.getUnconnectedOutLayersNames()\n",
    "            \n",
    "            #Runs forward pass to compute output of layer with name outputName\n",
    "            layerOutputs=net.forward(output_layer_names)\n",
    "            #model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\n",
    "            #There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\n",
    "            \n",
    "            #boxes stores the location properties of detected objects\n",
    "            #confidences stores the confidence score of detecting that object\n",
    "            boxes=[]\n",
    "            confidences=[]\n",
    "            appearance=[]\n",
    "            \n",
    "            #traversing through the 3 outputs at varying resolution\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                    #detection holds the location(first 5 elements) and class probabilities(rest 80 elements)\n",
    "                    scores=detection[5:]\n",
    "                    #print(scores)\n",
    "                    #selecting the index of maximum class probabilty of total 80 classes and its probability value as well\n",
    "                    class_id=np.argmax(scores)\n",
    "                    confidence=scores[class_id]\n",
    "                    #check to filter objects we are entirely sure belong to some class\n",
    "                    if(confidence>0.6):\n",
    "                        #Extracting the center coordinates of the bounding box\n",
    "                        #we need to convert back to original dimensions thus multiply by width,height is important\n",
    "                        #print(detection[0])\n",
    "                        center_x=detection[0]*width\n",
    "                        center_y=detection[1]*height\n",
    "                        #extracting the width and height of bounding box\n",
    "                        w=detection[2]*width\n",
    "                        h=detection[3]*height\n",
    "                        boxes.append([center_x,center_y,w,h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        appearance.append(scores)\n",
    "            \n",
    "            indexes=cv2.dnn.NMSBoxes(boxes,confidences,0.6,0.4)\n",
    "            #NMSBoxes=>https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
    "            i=0\n",
    "            if boxes!=[]:\n",
    "                \n",
    "                #extract flow area corresponsing to bounding box coordinates for detected objects\n",
    "                flows=get_flow_patches(flow,boxes,indexes)\n",
    "                if flows!=[]:\n",
    "                    for flow_patch in flows:\n",
    "                        mag, _ = cv2.cartToPolar(flow_patch[..., 0], flow_patch[..., 1])\n",
    "                        #computing mean, variance, kurtosis and skew of the magnitude of velocity vectors\n",
    "                        #motion metric\n",
    "                        mean=np.mean(mag)\n",
    "                        variance=np.var(mag)\n",
    "                        kurtosis_=kurtosis(mag,None)\n",
    "                        skew_=skew(mag,None)\n",
    "                        #location metric\n",
    "                        cx,cy,wi,hi=boxes[indexes.flatten()[i]]\n",
    "                        #appearance[indexes.flatten()[i]]-> appearance metric\n",
    "                        #now creating feature vector for the object and appending feature vector of given object to feature matrix along with their weight\n",
    "                        feature_matrix.append([mean,variance,kurtosis_,skew_]+[0.4*cx,0.4*cy,0.4*wi*hi]+list(0.9*appearance[indexes.flatten()[i]]))\n",
    "                        i=i+1\n",
    "                        \n",
    "                        \n",
    "            #updating previous frame            \n",
    "            frame_prev=frame\n",
    "            key=cv2.waitKey(1)\n",
    "            \n",
    "            #cv2.waitKey([delay])=>The function waitKey waits for a key event infinitely and the delay is in milliseconds. waitKey(0) means forever.\n",
    "            #For more details=>https://stackoverflow.com/questions/57690899/how-cv2-waitkey1-0xff-ordq-works\n",
    "            \n",
    "            #if pressed key has ASCII value 27 i.e q\n",
    "            if(key==27):\n",
    "                break\n",
    "            \n",
    "            #if len(indexes)>0:\n",
    "            #for i in indexes.flatten():                        \n",
    "            \n",
    "        else:\n",
    "            print(\"Error loading frame or End of frames\")\n",
    "            return\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc3ab3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading frame or End of frames\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f6e2bfc6625b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_direc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".avi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mvid_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_direc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#first using the AVENUE DATASET training videos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-586fe70faaef>\u001b[0m in \u001b[0;36mvid_process\u001b[1;34m(video_path, feature_matrix)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;31m#Runs forward pass to compute output of layer with name outputName\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mlayerOutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[1;31m#model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m#There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING THE DATASET\n",
    "train_vector=[]\n",
    "#Getting the parent directory\n",
    "parent_directory=os.path.dirname(os.getcwd())\n",
    "#os.path.dirname() method in Python is used to get directory name from the specified path \n",
    "#i.e the directory that hold the current file\n",
    "#Python method os.getcwd() returns current working directory of a process.\n",
    "#For more info->https://www.geeksforgeeks.org/python-os-path-dirname-method/\n",
    "\n",
    "#accesing the DATASET directory\n",
    "direc=os.path.join(parent_directory,'Dataset')\n",
    "#os.path.join-> https://www.geeksforgeeks.org/python-os-path-join-method/\n",
    "\n",
    "\n",
    "#first using the AVENUE DATASET training videos \n",
    "curr_direc=os.path.join(direc,'Avenue Dataset','training_videos')\n",
    "for filename in os.listdir(curr_direc):\n",
    "    #os.listdir->https://www.geeksforgeeks.org/python-os-listdir-method/\n",
    "    if filename.endswith(\".avi\"):\n",
    "        vid_process(os.path.join(curr_direc,filename),train_vector)\n",
    "        \n",
    "#Using the SHANGAI TECH DATASET training videos\n",
    "curr_direc=os.path.join(direc,'ShangaiTech_training','videos')\n",
    "for filename in os.listdir(curr_direc):\n",
    "    if filename.endswith(\".avi\"):\n",
    "        vid_process(os.path.join(curr_direc,filename),train_vector)\n",
    "        \n",
    "#using the PEDESTRIAN training dataset\n",
    "curr_direc=os.path.join(direc,'ped2','training','frames')\n",
    "for subdirec in os.listdir(curr_direc):\n",
    "    video_frames=os.path.join(curr_direc,subdirec)\n",
    "    frames_process(video_frames,train_vector)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.matlib\n",
    "def knndis(t,M2):\n",
    "    #A vectorised implementation of finding KNN distance between a single point and the M2 matrix\n",
    "    Mg=M2.shape[0]\n",
    "    #this will replicate the feature vector of 't' Mg times columnwise. \n",
    "    #To subtract, transpose is taken to make it compatible\n",
    "    #Sum is taken of each row\n",
    "    \n",
    "    dist=np.sqrt(np.sum((np.matlib.repmat(t,Mg,1) - M2)**2,1))\n",
    "    #np.matlib.repmat->Repeat given matrix columnwise(mg) and rowise(1)-> https://numpy.org/doc/stable/reference/generated/numpy.matlib.repmat.html\n",
    "    \n",
    "    #sorting the distance according to ascending order\n",
    "    np.sort(dist)\n",
    "    \n",
    "    #return the sum of first 10 neighbours i.e k=10\n",
    "    return sum(dist[0:10])/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d97a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2963, 87)\n"
     ]
    }
   ],
   "source": [
    "#randomly shuffling the feature matrix\n",
    "#print(train_vector)\n",
    "np.random.shuffle(train_vector)\n",
    "#random.shuffle-> shuffle contents of array inplace along the first axis of multi-array. \n",
    "#for more details->https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html\n",
    "\n",
    "#converting to np array for later modifications\n",
    "train_feature_matrix=np.array(train_vector)\n",
    "print(train_feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff319346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[414.2102715962895, 935.903451519551, 425.0768424777925, 494.88555532479825, 731.30979019236, 713.9259154909677, 508.72093629592484, 444.7699120390953, 770.3047040475385, 859.4395346614881, 562.6128778188842, 1055.6013999316528, 499.7554917539559, 1156.505520053372, 793.1003496894621, 499.5213595171538, 789.2314880313933, 461.18253853521446, 421.6490070491121, 405.05550669508773, 970.2867193032073, 470.59123865901586, 635.770825779061, 585.2695802237746, 777.0921094169938, 479.3319733189088, 849.6086819790447, 539.6322054771887, 817.819064646315, 581.7677483761414, 1043.1948122791077, 533.5035538443806, 487.9960561884688, 938.1448334547301, 1121.1554083188803, 1065.305158478737, 557.5589355440359, 455.37768865290155, 2748.0814153391902, 452.3731007551163, 442.4115323530724, 2541.8193631741533, 578.14505915621, 954.7484301427337, 3106.168510268943, 462.8645418985326, 425.2998151887111, 837.6724513610282, 1021.5149291822412, 833.3713419686321, 723.5883607873714, 877.6593626543996, 897.8678419516236, 871.9880926534049, 524.530970112139, 944.5821461353968, 493.95592189874816, 514.8302437459348, 2909.717221959565, 1051.6816864924394, 969.8447444463636, 609.7200027883478, 636.2478846376671, 696.8282563789353, 549.5763445761798, 464.99265822062824, 633.8325302434666, 1057.1136736658698, 424.92727375396373, 423.71105907020893, 664.711724210606, 667.0613988401831, 427.48577632573114, 1055.9932548559568, 528.0036805953417, 729.2964115879383, 407.5579509573435, 614.8833897578613, 678.0784721534823, 2437.8672857602446, 1218.7745141803648, 874.0644029333478, 1055.7058246720196, 623.8042506843453, 436.1241707024172, 493.93001585201927, 499.60418527693184, 469.4293875129797, 676.172082289775, 482.66708859071093, 867.6119837473219, 1415.432714108601, 536.3639896441169, 480.42020379055737, 428.15585022793584, 423.9167784500354, 561.1437265352646, 792.2346539312908, 901.5716900386005, 909.5116645132164, 522.0033264951446, 3421.847519874612, 415.5050660123792, 986.0523629156581, 614.5135876069817, 845.8070447902066, 535.0249814298109, 1384.9928729798419, 2207.3678227707733, 933.3584710987365, 740.6746205324829, 1216.1505710952047, 437.8760057293363, 665.6245825227472, 566.7013443572721, 404.07181175580024, 1013.4770811978731, 573.027282126697, 751.8096442688835, 448.3603756848729, 406.80457709581117, 760.9158563063171, 870.5191768268154, 474.18530501000225, 1163.5731796063835, 911.1245542301058, 1379.9564863336423, 847.6705580121383, 470.3236190503747, 428.8170767042746, 918.5122798442732, 413.8878239406793, 709.1834514141008, 425.9846788892022, 735.7545721071938, 871.6472447653832, 1227.3597347881591, 785.4424405477421, 684.5663293050142, 523.1160404606815, 490.1782621795848, 1136.8876277004379, 620.5542350107085, 457.9760942014844, 789.9158634398912, 1798.77011921175, 425.0429290184481, 415.4731455116586, 960.3839138746367, 575.4319551810254, 778.7263988597331, 680.3382260633555, 1031.6913958384994, 449.5645231794475, 785.538148813481, 423.30511366148846, 1178.5348980593506, 1103.1360447492018, 489.2043131917263, 419.2405572542708, 493.3332098463522, 443.6495477409926, 1257.783984211757, 476.1749935424058, 593.1883294338141, 516.8363969176561, 564.0751956444794, 433.34594724746, 783.8232460837723, 483.8540245518446, 678.4535041528669, 977.6155473995934, 418.67159790201765, 609.1420421415962, 915.6313666822331, 456.25233327096714, 544.0733297955329, 544.8262684723111, 1186.2098709099264, 452.23052042191495, 1005.6447424503087, 981.5822295578497, 456.2032309610656, 516.3484336879214, 1111.722303320647, 1099.968166372963, 647.1233609923437, 916.1527347151281, 432.38384561095506, 675.9704061164008, 469.4919137616627, 546.4936176790727, 776.6004814085845, 1211.3301476802153, 629.4890789403037, 934.9430459032453, 484.743317734538, 756.4961603503577, 1552.2089234933421, 1021.2259481028428, 1583.7946894888844, 873.9549325770804, 688.4275348504207, 1950.7214314416165, 1046.8286773148677, 444.027458984399, 445.8234289536651, 704.2186726486341, 635.6275100594638, 714.850488397055, 1080.0439119731877, 419.15149683461993, 700.0692766187442, 615.0844353934009, 618.310420779322, 1696.9751866587662, 770.6163850383673, 1036.245707974765, 455.99776896279826, 2369.4913286756723, 605.9142606524308, 628.6950137418604, 535.8931692768281, 1303.6041528875016, 1005.9199162429115, 739.2111355564614, 837.3295302009341, 1448.7413845016165, 1518.5128117922447, 407.4939018348876, 879.6404208144573, 507.65709288315054, 1235.584009492222, 1073.3546581813725, 527.4874619788116, 445.8809023314723, 651.9390708091103, 558.6366955560444, 1217.0332757201068, 591.1292574958579, 527.5381384626401, 413.2104925266394, 414.2470369965864, 1083.0483370364714, 3156.101256967951, 775.2192070931966, 460.5912184567716, 945.9250956535298, 1733.0755419049437, 709.3937084642309, 463.1911574517884, 1227.0926735931866, 1172.6070237064469, 2269.7926368787303, 546.87180817592, 739.8030549565473, 423.30735951313835, 443.8569420908473, 492.07654369644945, 485.6570340368404, 746.0841076750924, 886.5228511462681, 725.628346945118, 4036.0907730124463, 434.8577816825841, 588.7283174049168, 474.87243473621083, 427.5829341160417, 438.1980283578041, 492.6011033243375, 855.9207163591436, 471.5128475733201, 1013.3149920642849, 457.0459432477167, 423.46814545714824, 723.9387211146328, 575.8162167300386, 785.8137586713343, 564.2845407150359, 485.5953113486315, 440.36917242322943, 446.79906115846535, 583.7251698503691, 677.2673496358985, 749.750920233766, 787.2610761619972, 592.328702455901, 706.2128303120509, 410.84252080772904, 1686.3251231496827, 441.5559714027962, 2635.66548951661, 506.72676339192657, 691.2666001119735, 409.05664915313383, 657.3173517125675, 522.7141645363066, 405.52429695069014, 1709.1640367561854, 1041.0816394212204, 840.9000020050762, 884.5982824873633, 484.1227044000264, 658.1576362887834, 1282.6193047577858, 955.5649981938379, 4086.5979672329913, 1027.7032609241003, 1085.9373924508968, 556.0552995399702, 475.26672372428266, 520.1900308820008, 1216.9938867743376, 988.8537729728638, 1000.3352242515464, 653.4642686899232, 1195.8446378288781, 574.254322748541, 477.5852702194402, 561.0396006931736, 1118.040298628086, 1259.6854816055206, 1012.2640367421955, 776.7757949531489, 2732.8225065786696, 418.7127550729609, 688.4126245596697, 799.7594037422107, 593.471357052331, 522.209512540045, 441.261481300172, 1216.893484310221, 891.3184303995862, 1027.3128081206792, 853.5823089244684, 549.7319535752802, 598.5623300422085, 408.52172385536403, 511.74783514640694, 1030.8714487381815, 423.43744076483455, 485.95210386498854, 3803.9259969300283, 796.7586286727171, 636.0534677328955, 904.1086752864637, 1069.0624568917099, 724.5420272328516, 980.1582346609791, 621.8698687416065, 584.6964184618706, 923.2011275593793, 532.4553954947116, 990.8150423837282, 407.802037940373, 991.565918799159, 423.1218741005117, 438.40913581822235, 505.3819034412248, 480.5563310472288, 910.5630932176867, 1598.5544191341535, 417.3042085013473, 535.7795974857698, 1032.0328040194818, 788.1102052983418, 418.04914711154413, 958.9995233291917, 518.0992233271711, 865.9498801370626, 404.922822629777, 615.6427895056331, 407.18580765668116, 676.5563038956723, 924.9188364055601, 768.1726643495755, 410.48745668487373, 417.46230734701885, 913.6677142903633, 528.4678945588391, 504.51016747887587, 512.0509856261285, 617.0652042330787, 975.7956344300685, 499.22770649347177, 446.315576027346, 712.6449962984113, 538.3692104068514, 1754.865912889122, 495.19872912673384, 926.4173507251107, 905.6731880946566, 2878.9617073113013, 555.1128606155775, 1891.923016574009, 633.3465909795575, 2243.1589492705407, 857.1085230417448, 917.7592114488095, 491.96618273379363, 463.60661601154754, 468.1073974461495, 715.9493007081651, 404.67926259389503, 1029.3194398635894, 437.3590496290091, 451.9103029831613, 1179.7973566140076, 754.7487537274131, 439.37875993425644, 478.8916193563995, 437.59381158542055, 1001.0225697916864, 1182.8877853119525, 487.44739552912887, 2118.6478014914237, 1935.0496629612494, 903.1434779698072, 630.7878797612032, 2924.942267000851, 460.45820668154374, 412.2121468306585, 819.1399911042839, 498.58244438719885, 440.50670380836465, 719.6015934560231, 497.2568370319033, 690.2770561644513, 413.7595681053028, 1217.186754087995, 428.2912313662546, 443.5062441939306, 413.3097449839048, 999.2220496123164, 826.154898030804, 459.4505332514016, 499.60886711153887, 1811.1546917208775, 693.3113029967358, 769.8540692649509, 1163.3157345935356, 1021.7178004269506, 488.639238254294, 960.3652037165062, 953.4172112951334, 753.4035635503783, 974.030478337219, 443.55584913744707, 769.7097582280824, 440.6537793740408, 1004.9922695197711, 571.5490739239399, 1021.9143051503261, 976.6323197204063, 652.0103335413166, 1205.1760491391874, 825.9390211555453, 670.6511814053987, 567.4884349583806, 1199.2792311249718, 812.5428373782681, 461.2640336000397, 794.505200155089, 2434.7567246835993, 532.9341126835783, 1208.0032702831438, 942.4309989974436, 452.6637366181056, 645.2911195657407, 416.4620428884069, 1017.6732806247362, 1023.8661636675199, 1660.6298867735927, 416.4193520835267, 554.2598448023922, 662.2581057392696, 438.8526982256405, 796.2962886724399, 944.6380023317358, 428.10297330813484, 494.33250963950377, 1221.6524713720994, 420.7248412632437, 1575.3523913998922, 896.3902003376681, 788.5142746161503, 449.9953540668991, 634.1882799403122, 858.7938616101212, 465.8582355478928, 440.96398110148766, 711.6474559074231, 1201.9393469285255, 1098.5412531021402, 415.18631474872257, 2427.8987818619325, 551.644102322262, 1707.718846769679, 573.4083578857517, 525.3906045904602, 3560.8719714891276, 405.3299555878672, 450.3496048702491, 407.2040558193409, 482.62395208339984, 1211.812312362281, 423.8921347308362, 715.0058975257422, 476.83629738028594, 731.2155045304216, 1169.6945203914445, 903.2393335790821, 546.3923219414258, 998.2745713886949, 1219.8333729332176, 990.3996320319293, 1084.287152343212, 997.6883731248985, 850.5309342900595, 845.1485730227789, 2177.2620621811107, 882.0945050838912, 738.0493522074025, 476.90448941688567, 1640.9739644816964, 548.9095539932914, 431.9692325522739, 1171.5608717871442, 737.0052825077967, 668.6591392162512, 580.3969753878216, 773.5992212403946, 423.9245242769633, 1051.1541524880745, 482.3246778767433, 519.0579888326276, 2136.0744860300797, 479.4496735293881, 451.20138833475505, 418.48802162326484, 666.2841463664854, 704.1536591278493, 906.5264164628956, 3838.6774348605786, 451.378281260878, 485.05897751189, 424.3439562220965, 534.0365809447886, 903.9346891467978, 783.6324749568272, 3369.010935634616, 595.9147921665046, 740.899697405396, 421.4837801167735, 1680.3035365426774, 451.18055940314053, 518.4027834729783, 423.63454891805515, 406.0963564268671, 471.49929158412704, 728.1817132913839, 428.40867101425613, 446.7784256011225, 898.5753483811283, 477.4095301730549, 1208.0103897960603, 557.7866368504989, 1020.7641506739525, 1077.4441204182724, 543.292513477605, 1300.1904443398334, 1370.320449020191, 1043.9773782858192, 438.1865624667895, 1004.704710079867, 1032.5067430633162, 3505.248314450712, 610.6957074336872, 405.20300515563406, 449.7119204432637, 946.513490322789, 1288.5141782362462, 518.8133583586082, 435.2946873298791, 666.3545968426433, 840.3267960355815, 416.9433632162012, 1232.535266038061, 444.92599282125457, 795.7441188924716, 650.7285899739543, 416.13152179634136]\n"
     ]
    }
   ],
   "source": [
    "#Keeps track of di for all object i. \n",
    "errors=list()\n",
    "\n",
    "#as given in the paper, split the training dataset into M1 and M2. Here 20% of training dataset goes to M1. \n",
    "p1=int(train_feature_matrix.shape[0]*0.2)\n",
    "p2=train_feature_matrix.shape[0]-p1\n",
    "\n",
    "M1=train_feature_matrix[0:p1]\n",
    "M2=train_feature_matrix[p1:None]\n",
    "\n",
    "#Finding the KNN distance for all the points in M1\n",
    "for i in range(p1):\n",
    "    errors.append(knndis(M1[i],M2)) \n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e90c6207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909.5116645132164\n"
     ]
    }
   ],
   "source": [
    "#Using base thresold as one where 90% of KNN distance is lesser\n",
    "Base_lm = np.sort(errors)[int(len(errors)*0.7)]\n",
    "print(Base_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6749b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18b91902580>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plotting the KNN distance\n",
    "plt.plot(np.sort(errors)[1:1800])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
