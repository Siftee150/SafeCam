{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3765730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#openCV\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis,skew\n",
    "#scipy is the statistical elder brother of numpy\n",
    "import os\n",
    "#os helps in fetching files from different directories i.e. communication with the operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f80046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_vector:\n",
    "    #As given in  https://arxiv.org/abs/2004.07941, create the motion, location and appearance metric for the objects\n",
    "    motion=[]\n",
    "    location=[]\n",
    "    appearance=[]\n",
    "    \n",
    "    #weight corresponding to the motion metric\n",
    "    w1=1\n",
    "    \n",
    "    #weight corresponding to the location metric\n",
    "    w2=0.4\n",
    "    \n",
    "    #weight corresponding to the appearance metric\n",
    "    w3=0.9\n",
    "    \n",
    "    #constructor in python\n",
    "    def __init__(self,motion,location,appearance):\n",
    "        self.motion=motion\n",
    "        self.location=location\n",
    "        self.appearance=appearance\n",
    "        \n",
    "    #change weighting  \n",
    "    def set_weights(self,w11,w22,w33):\n",
    "        self.w1=w11\n",
    "        self.w2=w22\n",
    "        self.w3=w33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe97ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(img,boxes,idx):\n",
    "    #Extract a patch in the image corresponding to the object's bounding box in the frame.\n",
    "    imgs=[]\n",
    "    if len(idx)>0:\n",
    "        for i in idx.flatten():\n",
    "            center_x,center_y,w,h=boxes[i]\n",
    "            # Using the center x, y coordinates to derive the top\n",
    "            # and the left corner of the bounding box\n",
    "            x=int(center_x-(w/2))\n",
    "            y=int(center_y-(h/2))\n",
    "            ##\n",
    "            #The following is done to deal with typical situation when x or y are going out of frame\n",
    "            #i.e not whole bounding box in the frame\n",
    "            \n",
    "            #stores part of the width to be cropped which is out of frame\n",
    "            subx=0\n",
    "            #stores part of the height to be cropped which is out of frame\n",
    "            suby=0\n",
    "            \n",
    "            if x<0 or x>=img.shape[1]:\n",
    "                subx=x if x<0 else -1*x\n",
    "                x=0 if x<0 else img.shape[1]-1\n",
    "            if y<0 or y>=img.shape[0]:\n",
    "                suby=y if y<0 else -1*y\n",
    "                y=0 if y<0 else img.shape[0]-1\n",
    "            ##\n",
    "            imgs.append(img[y:y+int(h)+suby,x:x+int(w)+subx,:])\n",
    "    return imgs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c21343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_process(frames_path, feature_matrix):\n",
    "    frame_prev=None\n",
    "    for img in os.listdir(frames_path):\n",
    "        if img.endswith(\".jpg\"):\n",
    "            # RUNNING YOLOv4 OBJECT DETECTION FIRST\n",
    "            # returns a deep learning network using the yolov4 format\n",
    "            \n",
    "            net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
    "            # cv2.dnn.readNet=>https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422\n",
    "            # type net=Net object=>https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html\n",
    "            \n",
    "            # for running optical flow algorithm we need previous frame as well\n",
    "            if frame_prev is None:\n",
    "                frame_prev=cv2.imread(img)\n",
    "                continue\n",
    "            \n",
    "            frame=cv2.imread(img)\n",
    "            \n",
    "            (height, width, _) = frame.shape\n",
    "            \n",
    "            # preprocessing the frame before feeding it to the neural net.\n",
    "            # scale the pixel values to 1/255=>1/255\n",
    "            # Resizing frame to (416,416) pixels as yolov4 architecture works on frame of that size.\n",
    "            # No mean supplied to the three R,G,B channels=>(0,0,0)\n",
    "            # OpenCV assumes images are in BGR channel order, thus we must swap the R and B channels of the original RGB frame=> swapRB=true\n",
    "            # No cropping of the frame=>crop=False\n",
    "            blob = cv2.dnn.blobFromImage(frame,1 / 255,(416, 416),(0, 0, 0),swapRB=True,crop=False)\n",
    "            # above parameters are needed for yolov4 detection!\n",
    "            # cv2.dnn.blobFromImage=> creates processed 4-dimensional blob for use in our neural net.Further info=> https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "\n",
    "            # setting input to neural net\n",
    "            net.setInput(blob)\n",
    "\n",
    "            # net.getUnconnectedOutLayers(): It gives you the final layers number in the list from net.getLayerNames().\n",
    "            output_layer_names = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "            # Runs forward pass to compute output of layer with name outputName\n",
    "            layerOutputs = net.forward(output_layer_names)\n",
    "\n",
    "            # model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\n",
    "            # There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\n",
    "\n",
    "            # boxes stores the location properties of detected objects\n",
    "            # confidences stores the confidence score of detecting that object\n",
    "\n",
    "            boxes = []\n",
    "            confidences = []\n",
    "            appearance = []\n",
    "\n",
    "            # traversing through the 3 outputs at varying resolution\n",
    "\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                    # detection holds the location(first 5 elements) and class probabilities(rest 80 elements)\n",
    "                    scores = detection[5:]\n",
    "\n",
    "                    # selecting the index of maximum class probabilty of total 80 classes and its probability value as well\n",
    "\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    # check to filter objects we are entirely sure belong to some class\n",
    "\n",
    "                    if confidence > 0.6:\n",
    "\n",
    "                        # Extracting the center coordinates of the bounding box\n",
    "                        # we need to convert back to original dimensions thus multiply by width,height is important\n",
    "                        # print(detection[0])\n",
    "\n",
    "                        center_x = detection[0] * width\n",
    "                        center_y = detection[1] * height\n",
    "\n",
    "                        # extracting the width and height of bounding box\n",
    "\n",
    "                        w = detection[2] * width\n",
    "                        h = detection[3] * height\n",
    "                        boxes.append([center_x, center_y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        appearance.append(scores)\n",
    "\n",
    "            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.6, 0.4)\n",
    "\n",
    "            # NMSBoxes=>https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
    "\n",
    "            i = 0\n",
    "            if boxes != []:\n",
    "\n",
    "                # extract image corresponsing to bounding box coordinates for previous frame\n",
    "\n",
    "                patch_prev = get_patch(frame_prev, boxes, indexes)\n",
    "\n",
    "                # extract image corresponding to bounding box coordinates for current frame\n",
    "\n",
    "                patch = get_patch(frame, boxes, indexes)\n",
    "\n",
    "                if patch_prev != [] and patch != []:\n",
    "\n",
    "                    # looping through objects detected to find their OPTICAL FLOW\n",
    "\n",
    "                    for (prev, actual) in zip(patch_prev, patch):\n",
    "\n",
    "                        # Preprocessing to gray scale\n",
    "\n",
    "                        prvs = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "                        next_ = cv2.cvtColor(actual, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                        # ##TRIAL--flow = np.zeros([prev.shape[0],prev.shape[1],2])\n",
    "                        # ##TRIAL--cv2.optflow.calcOpticalFlowDenseRLOF(prev, actual,flow,None)\n",
    "\n",
    "                        # Dense optical flow algorithm\n",
    "\n",
    "                        flow = cv2.calcOpticalFlowFarneback(prvs,next_,None,0.5,3,15,3,5,1.2,0,)\n",
    "\n",
    "                        # getting magnitude of velocity vectors of optical flow\n",
    "\n",
    "                        (mag, _) = cv2.cartToPolar(flow[..., 0],flow[..., 1])\n",
    "\n",
    "                        # computing mean, variance, kurtosis and skew of the magnitude of velocity vectors\n",
    "\n",
    "                        mean = np.mean(mag)\n",
    "                        variance = np.var(mag)\n",
    "                        kurtosis_ = kurtosis(mag, None)\n",
    "                        skew_ = skew(mag, None)\n",
    "\n",
    "                        # now creating feature vector for the object\n",
    "                        # motion metric\n",
    "\n",
    "                        motion = [mean, variance, kurtosis_, skew_]\n",
    "                        (cx, cy, wi, hi) = boxes[indexes.flatten()[i]]\n",
    "\n",
    "                        # location metric\n",
    "\n",
    "                        location = [cx, cy, wi * hi]\n",
    "\n",
    "                        # appearance metric\n",
    "\n",
    "                        appear = appearance[indexes.flatten()[i]]\n",
    "\n",
    "                        # Appending feature vector of given object to feature matrix\n",
    "\n",
    "                        feature_matrix.append(Feature_vector(motion,location, appear))\n",
    "                        #print(feature_matrix)\n",
    "\n",
    "                        i = i + 1\n",
    "\n",
    "            # updating previous frame\n",
    "\n",
    "            frame_prev = frame\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db4358bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid_process(video_path,feature_matrix):\n",
    "    #get video capture object for the camera for the specified video file \n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    \n",
    "    #RUNNING YOLOv4 OBJECT DETECTION FIRST\n",
    "    \n",
    "    #returns a deep learning network using the yolov4 format \n",
    "    net=cv2.dnn.readNet('yolov4.weights','yolov4.cfg')\n",
    "    #cv2.dnn.readNet=>https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422\n",
    "    #type net=Net object=>https://docs.opencv.org/3.4/db/d30/classcv_1_1dnn_1_1Net.html\n",
    "    \n",
    "    #Sometimes, cap may not have initialized the capture. \n",
    "    #You can check whether it is initialized or not by the method cap.isOpened().\n",
    "    if(cap.isOpened()==False): \n",
    "        print(\"Error connecting to camera\")\n",
    "        return -1\n",
    "    \n",
    "    #for running optical flow algorithm we need previous frame as well\n",
    "    ret,frame_prev=cap.read()\n",
    "    #cap.read() returns a bool (True/False) which is stored in \"ret\" here. \n",
    "    #If the frame is read correctly, it will be True    \n",
    "    \n",
    "    if ret==False:\n",
    "        print(\"Error loading frame\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        ret,frame=cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            #storing original frame dimensions\n",
    "            height,width,_=frame.shape\n",
    "            \n",
    "            #preprocessing the frame before feeding it to the neural net. \n",
    "            #scale the pixel values to 1/255=>1/255\n",
    "            #Resizing frame to (416,416) pixels as yolov4 architecture works on frame of that size.\n",
    "            #No mean supplied to the three R,G,B channels=>(0,0,0)\n",
    "            #OpenCV assumes images are in BGR channel order, thus we must swap the R and B channels of the original RGB frame=> swapRB=true\n",
    "            #No cropping of the frame=>crop=False\n",
    "            blob=cv2.dnn.blobFromImage(frame,1/255,(416,416),(0,0,0),swapRB=True,crop=False)\n",
    "            #above parameters are needed for yolov4 detection!\n",
    "            #cv2.dnn.blobFromImage=> creates processed 4-dimensional blob for use in our neural net.Further info=> https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "            \n",
    "            #setting input to neural net\n",
    "            net.setInput(blob)\n",
    "            \n",
    "            #net.getUnconnectedOutLayers(): It gives you the final layers number in the list from net.getLayerNames().\n",
    "            output_layer_names=net.getUnconnectedOutLayersNames()\n",
    "            \n",
    "            #Runs forward pass to compute output of layer with name outputName\n",
    "            layerOutputs=net.forward(output_layer_names)\n",
    "            #model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\n",
    "            #There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\n",
    "            \n",
    "            #boxes stores the location properties of detected objects\n",
    "            #confidences stores the confidence score of detecting that object\n",
    "            boxes=[]\n",
    "            confidences=[]\n",
    "            appearance=[]\n",
    "            \n",
    "            #traversing through the 3 outputs at varying resolution\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                    #detection holds the location(first 5 elements) and class probabilities(rest 80 elements)\n",
    "                    scores=detection[5:]\n",
    "                    #print(scores)\n",
    "                    #selecting the index of maximum class probabilty of total 80 classes and its probability value as well\n",
    "                    class_id=np.argmax(scores)\n",
    "                    confidence=scores[class_id]\n",
    "                    #check to filter objects we are entirely sure belong to some class\n",
    "                    if(confidence>0.6):\n",
    "                        #Extracting the center coordinates of the bounding box\n",
    "                        #we need to convert back to original dimensions thus multiply by width,height is important\n",
    "                        #print(detection[0])\n",
    "                        center_x=detection[0]*width\n",
    "                        center_y=detection[1]*height\n",
    "                        #extracting the width and height of bounding box\n",
    "                        w=detection[2]*width\n",
    "                        h=detection[3]*height\n",
    "                        boxes.append([center_x,center_y,w,h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        appearance.append(scores)\n",
    "            \n",
    "            indexes=cv2.dnn.NMSBoxes(boxes,confidences,0.6,0.4)\n",
    "            #NMSBoxes=>https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
    "            i=0\n",
    "            if boxes!=[]:\n",
    "                #extract image corresponsing to bounding box coordinates for previous frame\n",
    "                patch_prev=get_patch(frame_prev,boxes,indexes)\n",
    "                #extract image corresponding to bounding box coordinates for current frame\n",
    "                patch=get_patch(frame,boxes,indexes)\n",
    "                \n",
    "                if patch_prev!=[] and patch!=[]:\n",
    "                    #looping through objects detected to find their OPTICAL FLOW\n",
    "                    for prev,actual in zip(patch_prev,patch):\n",
    "                        #Preprocessing to gray scale\n",
    "                        prvs  = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "                        next_ = cv2.cvtColor(actual, cv2.COLOR_BGR2GRAY)\n",
    "                        \n",
    "                        ###TRIAL--flow = np.zeros([prev.shape[0],prev.shape[1],2])\n",
    "                        ###TRIAL--cv2.optflow.calcOpticalFlowDenseRLOF(prev, actual,flow,None)\n",
    "                        \n",
    "                        #Dense optical flow algorithm\n",
    "                        flow=cv2.calcOpticalFlowFarneback(prvs, next_, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                        #getting magnitude of velocity vectors of optical flow \n",
    "                        mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                        #computing mean, variance, kurtosis and skew of the magnitude of velocity vectors\n",
    "                        mean=np.mean(mag)\n",
    "                        variance=np.var(mag)\n",
    "                        kurtosis_=kurtosis(mag,None)\n",
    "                        skew_=skew(mag,None)\n",
    "                        #now creating feature vector for the object\n",
    "                        \n",
    "                        #motion metric\n",
    "                        motion=[mean,variance,kurtosis_,skew_]\n",
    "                        cx,cy,wi,hi=boxes[indexes.flatten()[i]]\n",
    "                        #location metric\n",
    "                        location=[cx,cy,wi*hi]\n",
    "                        \n",
    "                        #appearance metric\n",
    "                        appear=appearance[indexes.flatten()[i]]\n",
    "                        \n",
    "                        #Appending feature vector of given object to feature matrix\n",
    "                        feature_matrix.append(Feature_vector(motion,location,appear))\n",
    "                        print(feature_matrix)\n",
    "                       \n",
    "                        i=i+1\n",
    "                        \n",
    "                        \n",
    "            #updating previous frame            \n",
    "            frame_prev=frame\n",
    "            key=cv2.waitKey(1)\n",
    "            \n",
    "            #cv2.waitKey([delay])=>The function waitKey waits for a key event infinitely and the delay is in milliseconds. waitKey(0) means forever.\n",
    "            #For more details=>https://stackoverflow.com/questions/57690899/how-cv2-waitkey1-0xff-ordq-works\n",
    "            \n",
    "            #if pressed key has ASCII value 27 i.e q\n",
    "            if(key==27):\n",
    "                break\n",
    "            \n",
    "            #if len(indexes)>0:\n",
    "            #for i in indexes.flatten():                        \n",
    "            \n",
    "        else:\n",
    "            print(\"Error loading frame\")\n",
    "            return\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc3ab3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Feature_vector object at 0x000001C07D060310>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>, <__main__.Feature_vector object at 0x000001C07D06AC10>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>, <__main__.Feature_vector object at 0x000001C07D06AC10>, <__main__.Feature_vector object at 0x000001C07F4B3F10>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>, <__main__.Feature_vector object at 0x000001C07D06AC10>, <__main__.Feature_vector object at 0x000001C07F4B3F10>, <__main__.Feature_vector object at 0x000001C07F47A520>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>, <__main__.Feature_vector object at 0x000001C07D06AC10>, <__main__.Feature_vector object at 0x000001C07F4B3F10>, <__main__.Feature_vector object at 0x000001C07F47A520>, <__main__.Feature_vector object at 0x000001C07F47A430>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>, <__main__.Feature_vector object at 0x000001C07D06AC10>, <__main__.Feature_vector object at 0x000001C07F4B3F10>, <__main__.Feature_vector object at 0x000001C07F47A520>, <__main__.Feature_vector object at 0x000001C07F47A430>, <__main__.Feature_vector object at 0x000001C07F47AEB0>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>, <__main__.Feature_vector object at 0x000001C07D06AC10>, <__main__.Feature_vector object at 0x000001C07F4B3F10>, <__main__.Feature_vector object at 0x000001C07F47A520>, <__main__.Feature_vector object at 0x000001C07F47A430>, <__main__.Feature_vector object at 0x000001C07F47AEB0>, <__main__.Feature_vector object at 0x000001C07F47A190>]\n",
      "[<__main__.Feature_vector object at 0x000001C07D060310>, <__main__.Feature_vector object at 0x000001C07D06A7F0>, <__main__.Feature_vector object at 0x000001C07D06AC10>, <__main__.Feature_vector object at 0x000001C07F4B3F10>, <__main__.Feature_vector object at 0x000001C07F47A520>, <__main__.Feature_vector object at 0x000001C07F47A430>, <__main__.Feature_vector object at 0x000001C07F47AEB0>, <__main__.Feature_vector object at 0x000001C07F47A190>, <__main__.Feature_vector object at 0x000001C07F47A340>]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e8a87d0fee41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#os.listdir->https://www.geeksforgeeks.org/python-os-listdir-method/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".avi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mvid_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Using the SHANGAI TECH DATASET training videos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-a3d5f6280bc8>\u001b[0m in \u001b[0;36mvid_process\u001b[1;34m(video_path, feature_matrix)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m#Runs forward pass to compute output of layer with name outputName\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mlayerOutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;31m#model output=>https://stackoverflow.com/questions/57112038/yolo-v3-model-output-clarification-with-keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#There are 3 output layers in YOLO for 3 different resolutions of grid boxes over which object is detected(13,13)(26,26)(52,52)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING THE DATASET\n",
    "train_vector=[]\n",
    "#Getting the parent directory\n",
    "parent_directory=os.path.dirname(os.getcwd())\n",
    "#os.path.dirname() method in Python is used to get directory name from the specified path \n",
    "#i.e the directory that hold the current file\n",
    "#Python method os.getcwd() returns current working directory of a process.\n",
    "#For more info->https://www.geeksforgeeks.org/python-os-path-dirname-method/\n",
    "\n",
    "#accesing the DATASET directory\n",
    "direc=os.path.join(parent_directory,'Dataset')\n",
    "#os.path.join-> https://www.geeksforgeeks.org/python-os-path-join-method/\n",
    "\n",
    "#first using the AVENUE DATASET training videos for training\n",
    "curr_direc=os.path.join(direc,'Avenue Dataset','training_videos')\n",
    "for filename in os.listdir(curr_direc):\n",
    "    #os.listdir->https://www.geeksforgeeks.org/python-os-listdir-method/\n",
    "    if filename.endswith(\".avi\"):\n",
    "        vid_process(filename,train_vector)\n",
    "\n",
    "#Using the SHANGAI TECH DATASET training videos\n",
    "curr_direc=os.path.join(direc,'ShangaiTech_training','videos')\n",
    "for filename in os.listdir(curr_direc):\n",
    "    if filename.endswith(\".avi\"):\n",
    "        vid_process(filename,train_vector)\n",
    "\n",
    "#using the PEDESTRIAN training dataset\n",
    "curr_direc=os.path.join(direc,'ped2','training','frames')\n",
    "for subdirec in os.listdir(curr_direc):\n",
    "    video_frames=os.path.join(curr_direc,subdirec)\n",
    "    frames_process(video_frames,train_vector)\n",
    "    \n",
    "\n",
    "print(train_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
